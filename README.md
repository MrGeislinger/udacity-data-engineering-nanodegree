```
Do the following steps in your README.md file.

Discuss the purpose of this database in context of the startup, Sparkify, and their analytical goals.
State and justify your database schema design and ETL pipeline.
[Optional] Provide example queries and results for song play analysis.

The README file includes a summary of the project, how to run the Python scripts, and an explanation of the files in the repository. Comments are used effectively and each function has a docstring.

```

# Data Lake Apache Spark Example Project

## Summary 

This project from Udacity's Data Engineering Nanodegree is to demonstrate how to
implement an ETL pipeline for a data lake using Apache Spark.

## The Scenario

We're working with a music streaming startup who has grown their user base and song database even more and wants to move their data warehouse to a data lake. Their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

We are tasked with building an ETL pipeline that extracts their data from S3, processes them using Spark, and loads the data back into S3 as a set of dimensional tables. This will allow their analytics team to continue finding insights in what songs their users are listening to.

## The Data

* Song data: s3://udacity-dend/song_data
* Log data: s3://udacity-dend/log_data

### Song data 

> The first dataset is a subset of real data from the [Million Song Dataset](https://labrosa.ee.columbia.edu/millionsong/).
> Each file is in JSON format and contains metadata about a song and the artist
> of that song. The files are partitioned by the first three letters of each 
> song's track ID. For example, here are filepaths to two files in this dataset.

- Source: [s3://udacity-dend/song_data](s3://udacity-dend/song_data)

### Log data

> The second dataset consists of log files in JSON format generated by this 
> [event simulator](https://github.com/Interana/eventsim) based on the songs in
> the dataset above. These simulate app activity logs from an imaginary music 
> streaming app based on configuration settings.

- Log Source: [s3://udacity-dend/log_data](s3://udacity-dend/log_data)


## Database Schema

One fact table **songplays** and four dimensional tables (**songs**, **artists**, **users**, **time**) were created following the the star schema principle. This allows easy analysis of clean data for future investigation of the data.

### Fact Table

> **songplays** - records in event data associated with song plays i.e. records with page NextSong

### Dimensional Tables

> users - users in the app

> songs - songs in music database

> artists - artists in music database

> time - timestamps of records in songplays broken down into specific units


## Files

### `dl.cfg`

This file contains credentials to connect your AWS instance. Note the values are left blank in this project for security.

### `etl.py`

This is the script that will run the full ETL process. It will read the songs & log data from the specified s3 bucket and then transform the data into the relevant tables detailed above. Finally it will load these tables as parquet files into the specified output (a different s3 bucket). 

## Running the Project 

1. Enter your credentials and relevant information in `dwh.cfg` so data tables can be written to a different s3 bucket.
2. Specify the output location (a different s3 bucket or local directory) for `output_data` in the `main()` function of `etl.py`
3. Finally we extract, load & transform the data via the ETL process by running `python etl.py`