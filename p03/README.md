# Data Warehouse Redshift Example Project

## Summary

This project from Udacity's Data Engineering Nanodegree is to demonstrate how
to implement a data warehouse (Redshift).

### The Scenario

We're working with a music streaming startup whose user and song databases have
grown (located in S3) and wants to put their data and processes onto the cloud.

We are tasked with building an ETL pipeline that extracts the data from S3, 
stages them in Redshift, and then transforms data into a set of tables for the
analytics team.


### The Data

* Song data: s3://udacity-dend/song_data
* Log data: s3://udacity-dend/log_data
    * Log data json path: s3://udacity-dend/log_json_path.json 

#### Song data 

> The first dataset is a subset of real data from the [Million Song Dataset](https://labrosa.ee.columbia.edu/millionsong/).
> Each file is in JSON format and contains metadata about a song and the artist
> of that song. The files are partitioned by the first three letters of each 
> song's track ID. For example, here are filepaths to two files in this dataset.

- Source: [s3://udacity-dend/song_data](s3://udacity-dend/song_data)

#### Log data

> The second dataset consists of log files in JSON format generated by this 
> [event simulator](https://github.com/Interana/eventsim) based on the songs in
> the dataset above. These simulate app activity logs from an imaginary music 
> streaming app based on configuration settings.

- Log Source: [s3://udacity-dend/log_data](s3://udacity-dend/log_data)
- Log json path: [s3://udacity-dend/log_json_path.json](s3://udacity-dend/log_json_path.json) 

### Database Schema

#### Fact Table

> **songplays** - records in event data associated with song plays i.e. records with page NextSong

* songplay_id 
* start_time 
* user_id 
* level 
* song_id 
* artist_id 
* session_id 
* location 
* user_agent

#### Dimension Tables

> users - users in the app

* user_id
* first_name
* last_name
* gender
* level

> songs - songs in music database

* song_id
* title
* artist_id
* year
* duration

> artists - artists in music database

* artist_id
* name
* location
* lattitude
* longitude

> time - timestamps of records in songplays broken down into specific units

* start_time
* hour
* day
* week
* month
* year
* weekday
    
## Running the Project

1. You must create an AWS Redshift cluster using an IAM Role with _AmazonS3ReadOnlyAccess_
    * Note more clusters will allow the ETL process to be done in parallel
2. Enter your credentials and relevant information in `dwh.cfg`
3. Tables must be created by running `python create_tables.py`
    * This will create tables to load the data into during the ETL process
4. Finally we load & transform the data via the ETL process by running `python etl.py`

> Note the `sql_queries.py` holds queries to create tables and load & transform
> data for the tables in the database.